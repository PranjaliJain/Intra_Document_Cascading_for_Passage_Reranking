{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains the following:\n",
    "# - IDCM model testing on TREC45 dataset \n",
    "# - ColBERT,BERTCAT and BERTDOT models to show comparison in efficiency between IDCM and these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type IDCM to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## IDCM Model for Testing\n",
    "\n",
    "from typing import Dict, Union\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import PreTrainedModel,PretrainedConfig\n",
    "\n",
    "pre_trained_model_name = \"sebastian-hofstaetter/idcm-distilbert-msmarco_doc\"\n",
    "\n",
    "class IDCM_Config(PretrainedConfig):\n",
    "    bert_model:str\n",
    "    # how many passages get scored by BERT \n",
    "    sample_n:int\n",
    "\n",
    "    # type of fast module\n",
    "    sample_context:str\n",
    "\n",
    "    # how many passages to take from bert to create the final score (usually the same as sample_n, but could be set to 1 for max-p)\n",
    "    top_k_chunks:int\n",
    "\n",
    "    # window size\n",
    "    chunk_size:int\n",
    "\n",
    "    # left and right overlap (added to each window)\n",
    "    overlap:int \n",
    "\n",
    "    padding_idx:int = 0\n",
    "\n",
    "class IDCM_InferenceOnly(PreTrainedModel):\n",
    "    '''\n",
    "    IDCM is a neural re-ranking model for long documents, it creates an intra-document cascade between a fast (CK) and a slow module (BERT_Cat)\n",
    "    This code is only usable for inference (we removed the training mechanism for simplicity)\n",
    "    '''\n",
    "\n",
    "    config_class = IDCM_Config\n",
    "    base_model_prefix = \"bert_model\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg) -> None:\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        #\n",
    "        # bert - scoring\n",
    "        #\n",
    "        if isinstance(cfg.bert_model, str):\n",
    "            self.bert_model = AutoModel.from_pretrained(cfg.bert_model)\n",
    "        else:\n",
    "            self.bert_model = cfg.bert_model\n",
    "\n",
    "        #\n",
    "        # final scoring (combination of bert scores)\n",
    "        #\n",
    "        self._classification_layer = torch.nn.Linear(self.bert_model.config.hidden_size, 1)\n",
    "        self.top_k_chunks = cfg.top_k_chunks\n",
    "        self.top_k_scoring = nn.Parameter(torch.full([1,self.top_k_chunks], 1, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "        #\n",
    "        # local self attention\n",
    "        #\n",
    "        self.padding_idx= cfg.padding_idx\n",
    "        self.chunk_size = cfg.chunk_size\n",
    "        self.overlap = cfg.overlap\n",
    "        self.extended_chunk_size = self.chunk_size + 2 * self.overlap\n",
    "\n",
    "        #\n",
    "        # sampling stuff\n",
    "        #\n",
    "        self.sample_n = cfg.sample_n\n",
    "        self.sample_context = cfg.sample_context\n",
    "\n",
    "        if self.sample_context == \"ck\":\n",
    "            i = 3\n",
    "            self.sample_cnn3 = nn.Sequential(\n",
    "                        nn.ConstantPad1d((0,i - 1), 0),\n",
    "                        nn.Conv1d(kernel_size=i, in_channels=self.bert_model.config.dim, out_channels=self.bert_model.config.dim),\n",
    "                        nn.ReLU()\n",
    "                        ) \n",
    "        elif self.sample_context == \"ck-small\":\n",
    "            i = 3\n",
    "            self.sample_projector = nn.Linear(self.bert_model.config.dim,384)\n",
    "            self.sample_cnn3 = nn.Sequential(\n",
    "                        nn.ConstantPad1d((0,i - 1), 0),\n",
    "                        nn.Conv1d(kernel_size=i, in_channels=384, out_channels=128),\n",
    "                        nn.ReLU()\n",
    "                        ) \n",
    "\n",
    "        self.sampling_binweights = nn.Linear(11, 1, bias=True)\n",
    "        torch.nn.init.uniform_(self.sampling_binweights.weight, -0.01, 0.01)\n",
    "        self.kernel_alpha_scaler = nn.Parameter(torch.full([1,1,11], 1, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "        self.register_buffer(\"mu\",nn.Parameter(torch.tensor([1.0, 0.9, 0.7, 0.5, 0.3, 0.1, -0.1, -0.3, -0.5, -0.7, -0.9]), requires_grad=False).view(1, 1, 1, -1))\n",
    "        self.register_buffer(\"sigma\", nn.Parameter(torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]), requires_grad=False).view(1, 1, 1, -1))\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                query: Dict[str, torch.LongTensor],\n",
    "                document: Dict[str, torch.LongTensor],\n",
    "                use_fp16:bool = True,\n",
    "                output_secondary_output: bool = False):\n",
    "\n",
    "        #\n",
    "        # patch up documents - local self attention\n",
    "        #\n",
    "        document_ids = document[\"input_ids\"][:,1:]\n",
    "        if document_ids.shape[1] > self.overlap:\n",
    "            needed_padding = self.extended_chunk_size - (((document_ids.shape[1]) % self.chunk_size)  - self.overlap)\n",
    "        else:\n",
    "            needed_padding = self.extended_chunk_size - self.overlap - document_ids.shape[1]\n",
    "        orig_doc_len = document_ids.shape[1]\n",
    "\n",
    "        document_ids = nn.functional.pad(document_ids,(self.overlap, needed_padding),value=self.padding_idx)\n",
    "        chunked_ids = document_ids.unfold(1,self.extended_chunk_size,self.chunk_size)\n",
    "\n",
    "        batch_size = chunked_ids.shape[0]\n",
    "        chunk_pieces = chunked_ids.shape[1]\n",
    "\n",
    "\n",
    "        chunked_ids_unrolled=chunked_ids.reshape(-1,self.extended_chunk_size)\n",
    "        packed_indices = (chunked_ids_unrolled[:,self.overlap:-self.overlap] != self.padding_idx).any(-1)\n",
    "        orig_packed_indices = packed_indices.clone()\n",
    "        ids_packed = chunked_ids_unrolled[packed_indices]\n",
    "        mask_packed = (ids_packed != self.padding_idx)\n",
    "\n",
    "        total_chunks=chunked_ids_unrolled.shape[0]\n",
    "\n",
    "        packed_query_ids = query[\"input_ids\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"input_ids\"].shape[1])[packed_indices]\n",
    "        packed_query_mask = query[\"attention_mask\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"attention_mask\"].shape[1])[packed_indices]\n",
    "\n",
    "        #\n",
    "        # sampling\n",
    "        # \n",
    "        if self.sample_n > -1:\n",
    "            \n",
    "            #\n",
    "            # ck learned matches\n",
    "            #\n",
    "            if self.sample_context == \"ck-small\":\n",
    "                query_ctx = torch.nn.functional.normalize(self.sample_cnn3(self.sample_projector(self.bert_model.embeddings(packed_query_ids).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
    "                document_ctx = torch.nn.functional.normalize(self.sample_cnn3(self.sample_projector(self.bert_model.embeddings(ids_packed).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
    "            elif self.sample_context == \"ck\":\n",
    "                query_ctx = torch.nn.functional.normalize(self.sample_cnn3((self.bert_model.embeddings(packed_query_ids).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
    "                document_ctx = torch.nn.functional.normalize(self.sample_cnn3((self.bert_model.embeddings(ids_packed).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
    "            else:\n",
    "                qe = self.tk_projector(self.bert_model.embeddings(packed_query_ids).detach())\n",
    "                de = self.tk_projector(self.bert_model.embeddings(ids_packed).detach())\n",
    "                query_ctx = self.tk_contextualizer(qe.transpose(1,0),src_key_padding_mask=~packed_query_mask.bool()).transpose(1,0)\n",
    "                document_ctx = self.tk_contextualizer(de.transpose(1,0),src_key_padding_mask=~mask_packed.bool()).transpose(1,0)\n",
    "        \n",
    "                query_ctx =   torch.nn.functional.normalize(query_ctx,p=2,dim=-1)\n",
    "                document_ctx= torch.nn.functional.normalize(document_ctx,p=2,dim=-1)\n",
    "\n",
    "            cosine_matrix = torch.bmm(query_ctx,document_ctx.transpose(-1, -2)).unsqueeze(-1)\n",
    "\n",
    "            kernel_activations = torch.exp(- torch.pow(cosine_matrix - self.mu, 2) / (2 * torch.pow(self.sigma, 2))) * mask_packed.unsqueeze(-1).unsqueeze(1)\n",
    "            kernel_res = torch.log(torch.clamp(torch.sum(kernel_activations, 2) * self.kernel_alpha_scaler, min=1e-4)) * packed_query_mask.unsqueeze(-1)\n",
    "            packed_patch_scores = self.sampling_binweights(torch.sum(kernel_res, 1))\n",
    "\n",
    "            \n",
    "            sampling_scores_per_doc = torch.zeros((total_chunks,1), dtype=packed_patch_scores.dtype, layout=packed_patch_scores.layout, device=packed_patch_scores.device)\n",
    "            sampling_scores_per_doc[packed_indices] = packed_patch_scores\n",
    "            sampling_scores_per_doc = sampling_scores_per_doc.reshape(batch_size,-1,)\n",
    "            sampling_scores_per_doc_orig = sampling_scores_per_doc.clone()\n",
    "            sampling_scores_per_doc[sampling_scores_per_doc == 0] = -9000\n",
    "\n",
    "            sampling_sorted = sampling_scores_per_doc.sort(descending=True)\n",
    "            sampled_indices = sampling_sorted.indices + torch.arange(0,sampling_scores_per_doc.shape[0]*sampling_scores_per_doc.shape[1],sampling_scores_per_doc.shape[1],device=sampling_scores_per_doc.device).unsqueeze(-1)\n",
    "\n",
    "            sampled_indices = sampled_indices[:,:self.sample_n]\n",
    "            sampled_indices_mask = torch.zeros_like(packed_indices).scatter(0, sampled_indices.reshape(-1), 1)\n",
    "\n",
    "            # pack indices\n",
    "\n",
    "            packed_indices = sampled_indices_mask * packed_indices\n",
    "    \n",
    "            packed_query_ids = query[\"input_ids\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"input_ids\"].shape[1])[packed_indices]\n",
    "            packed_query_mask = query[\"attention_mask\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"attention_mask\"].shape[1])[packed_indices]\n",
    "\n",
    "            ids_packed = chunked_ids_unrolled[packed_indices]\n",
    "            mask_packed = (ids_packed != self.padding_idx)\n",
    "\n",
    "        #\n",
    "        # expensive bert scores\n",
    "        #\n",
    "        \n",
    "        bert_vecs = self.forward_representation(torch.cat([packed_query_ids,ids_packed],dim=1),torch.cat([packed_query_mask,mask_packed],dim=1))\n",
    "        packed_patch_scores = self._classification_layer(bert_vecs) \n",
    "\n",
    "        scores_per_doc = torch.zeros((total_chunks,1), dtype=packed_patch_scores.dtype, layout=packed_patch_scores.layout, device=packed_patch_scores.device)\n",
    "        scores_per_doc[packed_indices] = packed_patch_scores\n",
    "        scores_per_doc = scores_per_doc.reshape(batch_size,-1,)\n",
    "        scores_per_doc_orig = scores_per_doc.clone()\n",
    "        scores_per_doc_orig_sorter = scores_per_doc.clone()\n",
    "\n",
    "        if self.sample_n > -1:\n",
    "            scores_per_doc = scores_per_doc * sampled_indices_mask.view(batch_size,-1)\n",
    "        \n",
    "        #\n",
    "        # aggregate bert scores\n",
    "        #\n",
    "\n",
    "        if scores_per_doc.shape[1] < self.top_k_chunks:\n",
    "            scores_per_doc = nn.functional.pad(scores_per_doc,(0, self.top_k_chunks - scores_per_doc.shape[1]))\n",
    "\n",
    "        scores_per_doc[scores_per_doc == 0] = -9000\n",
    "        scores_per_doc_orig_sorter[scores_per_doc_orig_sorter == 0] = -9000\n",
    "        score = torch.sort(scores_per_doc,descending=True,dim=-1).values\n",
    "        score[score <= -8900] = 0\n",
    "\n",
    "        score = (score[:,:self.top_k_chunks] * self.top_k_scoring).sum(dim=1)\n",
    "\n",
    "        if self.sample_n == -1:\n",
    "            if output_secondary_output:\n",
    "                return score,{\n",
    "                    \"packed_indices\": orig_packed_indices.view(batch_size,-1),\n",
    "                    \"bert_scores\":scores_per_doc_orig\n",
    "                }\n",
    "            else:\n",
    "                return score,scores_per_doc_orig    \n",
    "        else:\n",
    "            if output_secondary_output:\n",
    "                return score,scores_per_doc_orig,{\n",
    "                    \"score\": score,\n",
    "                    \"packed_indices\": orig_packed_indices.view(batch_size,-1),\n",
    "                    \"sampling_scores\":sampling_scores_per_doc_orig,\n",
    "                    \"bert_scores\":scores_per_doc_orig\n",
    "                }\n",
    "\n",
    "            return score\n",
    "\n",
    "    def forward_representation(self, ids,mask,type_ids=None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        if self.bert_model.base_model_prefix == 'distilbert': # diff input / output \n",
    "            pooled = self.bert_model(input_ids=ids,\n",
    "                                     attention_mask=mask)[0][:,0,:]\n",
    "        elif self.bert_model.base_model_prefix == 'longformer':\n",
    "            _, pooled = self.bert_model(input_ids=ids,\n",
    "                                        attention_mask=mask.long(),\n",
    "                                        global_attention_mask = ((1-ids)*mask).long())\n",
    "        elif self.bert_model.base_model_prefix == 'roberta': # no token type ids\n",
    "            _, pooled = self.bert_model(input_ids=ids,\n",
    "                                        attention_mask=mask)\n",
    "        else:\n",
    "            _, pooled = self.bert_model(input_ids=ids,\n",
    "                                        token_type_ids=type_ids,\n",
    "                                        attention_mask=mask)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "\n",
    "## IDCM Tokenizer and Model\n",
    "idcm_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # honestly not sure if that is the best way to go, but it works :)\n",
    "idcm_model = IDCM_InferenceOnly.from_pretrained(pre_trained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## BERT-CAT Model\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModel, PreTrainedModel,PretrainedConfig\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "class BERT_Cat_Config(PretrainedConfig):\n",
    "    model_type = \"BERT_Cat\"\n",
    "    bert_model: str\n",
    "    trainable: bool = True\n",
    "\n",
    "class BERT_Cat(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    The vanilla/mono BERT concatenated (we lovingly refer to as BERT_Cat) architecture \n",
    "    -> requires input concatenation before model, so that batched input is possible\n",
    "    \"\"\"\n",
    "    config_class = BERT_Cat_Config\n",
    "    base_model_prefix = \"bert_model\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg) -> None:\n",
    "        super().__init__(cfg)\n",
    "        \n",
    "        self.bert_model = AutoModel.from_pretrained(cfg.bert_model)\n",
    "\n",
    "        for p in self.bert_model.parameters():\n",
    "            p.requires_grad = cfg.trainable\n",
    "\n",
    "        self._classification_layer = torch.nn.Linear(self.bert_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self,\n",
    "                query_n_doc_sequence):\n",
    "\n",
    "        vecs = self.bert_model(**query_n_doc_sequence)[0][:,0,:] # assuming a distilbert model here\n",
    "        score = self._classification_layer(vecs)\n",
    "        return score\n",
    "\n",
    "#\n",
    "# init the model & tokenizer (using the distilbert tokenizer)\n",
    "#\n",
    "bertcat_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # honestly not sure if that is the best way to go, but it works :)\n",
    "bertcat_model = BERT_Cat.from_pretrained(\"sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## COLBERT Model\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModel, PreTrainedModel,PretrainedConfig\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "class ColBERTConfig(PretrainedConfig):\n",
    "    model_type = \"ColBERT\"\n",
    "    bert_model: str\n",
    "    compression_dim: int = 768\n",
    "    dropout: float = 0.0\n",
    "    return_vecs: bool = False\n",
    "    trainable: bool = True\n",
    "\n",
    "class ColBERT(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    ColBERT model from: https://arxiv.org/pdf/2004.12832.pdf\n",
    "    We use a dot-product instead of cosine per term (slightly better)\n",
    "    \"\"\"\n",
    "    config_class = ColBERTConfig\n",
    "    base_model_prefix = \"bert_model\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg) -> None:\n",
    "        super().__init__(cfg)\n",
    "        \n",
    "        self.bert_model = AutoModel.from_pretrained(cfg.bert_model)\n",
    "\n",
    "        for p in self.bert_model.parameters():\n",
    "            p.requires_grad = cfg.trainable\n",
    "\n",
    "        self.compressor = torch.nn.Linear(self.bert_model.config.hidden_size, cfg.compression_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                query: Dict[str, torch.LongTensor],\n",
    "                document: Dict[str, torch.LongTensor]):\n",
    "\n",
    "        query_vecs = self.forward_representation(query)\n",
    "        document_vecs = self.forward_representation(document)\n",
    "\n",
    "        score = self.forward_aggregation(query_vecs,document_vecs,query[\"attention_mask\"],document[\"attention_mask\"])\n",
    "        return score\n",
    "\n",
    "    def forward_representation(self,\n",
    "                               tokens,\n",
    "                               sequence_type=None) -> torch.Tensor:\n",
    "        \n",
    "        vecs = self.bert_model(**tokens)[0] # assuming a distilbert model here\n",
    "        vecs = self.compressor(vecs)\n",
    "\n",
    "        # if encoding only, zero-out the mask values so we can compress storage\n",
    "        if sequence_type == \"doc_encode\" or sequence_type == \"query_encode\": \n",
    "            vecs = vecs * tokens[\"tokens\"][\"mask\"].unsqueeze(-1)\n",
    "\n",
    "        return vecs\n",
    "\n",
    "    def forward_aggregation(self,query_vecs, document_vecs,query_mask,document_mask):\n",
    "        \n",
    "        # create initial term-x-term scores (dot-product)\n",
    "        score = torch.bmm(query_vecs, document_vecs.transpose(2,1))\n",
    "\n",
    "        # mask out padding on the doc dimension (mask by -1000, because max should not select those, setting it to 0 might select them)\n",
    "        exp_mask = document_mask.bool().unsqueeze(1).expand(-1,score.shape[1],-1)\n",
    "        score[~exp_mask] = - 10000\n",
    "\n",
    "        # max pooling over document dimension\n",
    "        score = score.max(-1).values\n",
    "\n",
    "        # mask out paddding query values\n",
    "        score[~(query_mask.bool())] = 0\n",
    "\n",
    "        # sum over query values\n",
    "        score = score.sum(-1)\n",
    "\n",
    "        return score\n",
    "\n",
    "#\n",
    "# init the model & tokenizer (using the distilbert tokenizer)\n",
    "#\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # honestly not sure if that is the best way to go, but it works :)\n",
    "colbert_model = ColBERT.from_pretrained(\"sebastian-hofstaetter/colbert-distilbert-margin_mse-T2-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERT-DOT Model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# you can switch the model to the original \"distilbert-base-uncased\" to see that the usage example then breaks and the score ordering is reversed :O\n",
    "#pre_trained_model_name = \"distilbert-base-uncased\"\n",
    "pre_trained_model_name = \"sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco\"\n",
    "\n",
    "bertdot_tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name) \n",
    "bertdot_model = AutoModel.from_pretrained(pre_trained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unrolled_to_ranked_result(unrolled_results):\n",
    "    ranked_result = {}\n",
    "    for query_id, query_data in unrolled_results.items():\n",
    "        local_list = []\n",
    "        # sort the results per query based on the output\n",
    "        for (doc_id, output_value) in sorted(query_data, key=lambda x: x[1], reverse=True):\n",
    "            local_list.append(doc_id)\n",
    "        ranked_result[query_id] = local_list\n",
    "    return ranked_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_plain(ranking, qrels,binarization_point=1.0,return_per_query=False):\n",
    "    '''\n",
    "    calculate main evaluation metrics for the given results (without looking at candidates),\n",
    "    returns a dict of metrics\n",
    "    '''\n",
    "\n",
    "    ranked_queries = len(ranking)\n",
    "    ap_per_candidate_depth = np.zeros((ranked_queries))\n",
    "    #coverage_per_candidate_depth = np.zeros((len(global_metric_config[\"nDCG@\"]),ranked_queries))\n",
    "    rr_per_candidate_depth = np.zeros((len(global_metric_config[\"MRR+Recall@\"]),ranked_queries))\n",
    "    rank_per_candidate_depth = np.zeros((len(global_metric_config[\"MRR+Recall@\"]),ranked_queries))\n",
    "    recall_per_candidate_depth = np.zeros((len(global_metric_config[\"MRR+Recall@\"]),ranked_queries))\n",
    "    ndcg_per_candidate_depth = np.zeros((len(global_metric_config[\"nDCG@\"]),ranked_queries))\n",
    "    evaluated_queries = 0\n",
    "\n",
    "    for query_index,(query_id,ranked_doc_ids) in enumerate(ranking.items()):\n",
    "        \n",
    "        if query_id in qrels:\n",
    "            evaluated_queries += 1\n",
    "\n",
    "            relevant_ids = np.array(list(qrels[query_id].keys())) # key, value guaranteed in same order\n",
    "            relevant_grades = np.array(list(qrels[query_id].values()))\n",
    "            sorted_relevant_grades = np.sort(relevant_grades)[::-1]\n",
    "\n",
    "            num_relevant = relevant_ids.shape[0]\n",
    "            np_rank = np.array(ranked_doc_ids)\n",
    "            relevant_mask = np.in1d(np_rank,relevant_ids) # shape: (ranking_depth,) - type: bool\n",
    "\n",
    "            binary_relevant = relevant_ids[relevant_grades >= binarization_point]\n",
    "            binary_num_relevant = binary_relevant.shape[0]\n",
    "            binary_relevant_mask = np.in1d(np_rank,binary_relevant) # shape: (ranking_depth,) - type: bool\n",
    "\n",
    "            # check if we have a relevant document at all in the results -> if not skip and leave 0 \n",
    "            if np.any(binary_relevant_mask):\n",
    "                \n",
    "                # now select the relevant ranks across the fixed ranks\n",
    "                ranks = np.arange(1,binary_relevant_mask.shape[0]+1)[binary_relevant_mask]\n",
    "\n",
    "                #\n",
    "                # ap\n",
    "                #\n",
    "                map_ranks = ranks[ranks <= global_metric_config[\"MAP@\"]]\n",
    "                ap = np.arange(1,map_ranks.shape[0]+1) / map_ranks\n",
    "                ap = np.sum(ap) / binary_num_relevant\n",
    "                ap_per_candidate_depth[query_index] = ap\n",
    "                \n",
    "                # mrr only the first relevant rank is used\n",
    "                first_rank = ranks[0]\n",
    "\n",
    "                for cut_indx, cutoff in enumerate(global_metric_config[\"MRR+Recall@\"]):\n",
    "\n",
    "                    curr_ranks = ranks.copy()\n",
    "                    curr_ranks[curr_ranks > cutoff] = 0 \n",
    "\n",
    "                    recall = (curr_ranks > 0).sum(axis=0) / binary_num_relevant\n",
    "                    recall_per_candidate_depth[cut_indx,query_index] = recall\n",
    "\n",
    "                    #\n",
    "                    # mrr\n",
    "                    #\n",
    "\n",
    "                    # ignore ranks that are out of the interest area (leave 0)\n",
    "                    if first_rank <= cutoff: \n",
    "                        rr_per_candidate_depth[cut_indx,query_index] = 1 / first_rank\n",
    "                        rank_per_candidate_depth[cut_indx,query_index] = first_rank\n",
    "            \n",
    "            if np.any(relevant_mask):\n",
    "                \n",
    "                # now select the relevant ranks across the fixed ranks\n",
    "                ranks = np.arange(1,relevant_mask.shape[0]+1)[relevant_mask]\n",
    "\n",
    "                grades_per_rank = np.ndarray(ranks.shape[0],dtype=int)\n",
    "                for i,id in enumerate(np_rank[relevant_mask]):\n",
    "                    grades_per_rank[i]=np.where(relevant_ids==id)[0]\n",
    "\n",
    "                grades_per_rank = relevant_grades[grades_per_rank]\n",
    "\n",
    "                #\n",
    "                # ndcg = dcg / idcg \n",
    "                #\n",
    "                for cut_indx, cutoff in enumerate(global_metric_config[\"nDCG@\"]):\n",
    "                    #\n",
    "                    # get idcg (from relevant_ids)\n",
    "                    idcg = (sorted_relevant_grades[:cutoff] / np.log2(1 + np.arange(1,min(num_relevant,cutoff) + 1)))\n",
    "\n",
    "                    curr_ranks = ranks.copy()\n",
    "                    curr_ranks[curr_ranks > cutoff] = 0 \n",
    "\n",
    "                    #coverage_per_candidate_depth[cut_indx, query_index] = (curr_ranks > 0).sum() / float(cutoff)\n",
    "\n",
    "                    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                        c = np.true_divide(grades_per_rank,np.log2(1 + curr_ranks))\n",
    "                        c[c == np.inf] = 0\n",
    "                        dcg = np.nan_to_num(c)\n",
    "\n",
    "                    nDCG = dcg.sum(axis=-1) / idcg.sum()\n",
    "\n",
    "                    ndcg_per_candidate_depth[cut_indx,query_index] = nDCG\n",
    "\n",
    "    #avg_coverage = coverage_per_candidate_depth.sum(axis=-1) / evaluated_queries\n",
    "    mrr = rr_per_candidate_depth.sum(axis=-1) / evaluated_queries\n",
    "    relevant = (rr_per_candidate_depth > 0).sum(axis=-1)\n",
    "    non_relevant = (rr_per_candidate_depth == 0).sum(axis=-1)\n",
    "\n",
    "    avg_rank=np.apply_along_axis(lambda v: np.mean(v[np.nonzero(v)]), -1, rank_per_candidate_depth)\n",
    "    avg_rank[np.isnan(avg_rank)]=0.\n",
    "\n",
    "    median_rank=np.apply_along_axis(lambda v: np.median(v[np.nonzero(v)]), -1, rank_per_candidate_depth)\n",
    "    median_rank[np.isnan(median_rank)]=0.\n",
    "\n",
    "    map_score = ap_per_candidate_depth.sum(axis=-1) / evaluated_queries\n",
    "    recall = recall_per_candidate_depth.sum(axis=-1) / evaluated_queries\n",
    "    nDCG = ndcg_per_candidate_depth.sum(axis=-1) / evaluated_queries\n",
    "\n",
    "    local_dict={}\n",
    "\n",
    "    for cut_indx, cutoff in enumerate(global_metric_config[\"MRR+Recall@\"]):\n",
    "\n",
    "        local_dict['MRR@'+str(cutoff)] = mrr[cut_indx]\n",
    "        local_dict['Recall@'+str(cutoff)] = recall[cut_indx]\n",
    "        local_dict['QueriesWithNoRelevant@'+str(cutoff)] = non_relevant[cut_indx]\n",
    "        local_dict['QueriesWithRelevant@'+str(cutoff)] = relevant[cut_indx]\n",
    "        local_dict['AverageRankGoldLabel@'+str(cutoff)] = avg_rank[cut_indx]\n",
    "        local_dict['MedianRankGoldLabel@'+str(cutoff)] = median_rank[cut_indx]\n",
    "    \n",
    "    for cut_indx, cutoff in enumerate(global_metric_config[\"nDCG@\"]):\n",
    "        #local_dict['Avg_coverage@'+str(cutoff)] = avg_coverage[cut_indx]\n",
    "        local_dict['nDCG@'+str(cutoff)] = nDCG[cut_indx]\n",
    "\n",
    "    local_dict['QueriesRanked'] = evaluated_queries\n",
    "    local_dict['MAP@'+str(global_metric_config[\"MAP@\"])] = map_score\n",
    "    \n",
    "    if return_per_query:\n",
    "        return local_dict,rr_per_candidate_depth,ap_per_candidate_depth,recall_per_candidate_depth,ndcg_per_candidate_depth\n",
    "    else:\n",
    "        return local_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(path):\n",
    "    with open(path,'r') as f:\n",
    "        qids_to_relevant_passageids = {}\n",
    "        for l in f:\n",
    "            try:\n",
    "                l = l.strip().split()\n",
    "                qid = l[0]\n",
    "                if float(l[3]) > 0.0001:\n",
    "                    if qid not in qids_to_relevant_passageids:\n",
    "                        qids_to_relevant_passageids[qid] = {}\n",
    "                    qids_to_relevant_passageids[qid][l[2]] = float(l[3])\n",
    "            except:\n",
    "                raise IOError('\\\"%s\\\" is not valid format' % l)\n",
    "        return qids_to_relevant_passageids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_metric_config = {\n",
    "    \"MRR+Recall@\":[10,20,100,200,1000], # multiple allowed\n",
    "    \"nDCG@\":[3,5,10,20,1000], # multiple allowed\n",
    "    \"MAP@\":1000, #only one allowed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config={}\n",
    "test_config[\"binarization_point\"]=1\n",
    "test_config[\"qrels\"] = \"data/qrels.trec6-8.nocr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (1,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## Load TREC45 documents into a dataframe\n",
    "docs = pd.read_csv(\"data/trec45-documents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docnos = docs[\"DOCNO\"]\n",
    "doctext = docs[\"TEXT\"]\n",
    "docs = docs.drop(columns=['HEADLINE','GRAPHIC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TREC45 queries for testing (300-450)\n",
    "\n",
    "query_file = open(\"data/queries.txt\",\"r\").readlines()\n",
    "queries ={}\n",
    "for query in query_file:\n",
    "    lst = query.split(\" \",1)\n",
    "    queries[int(lst[0])] = lst[1].strip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top-100 BM25 documents for all queries -- obtained from Anserini\n",
    "bm25 = pd.read_csv(\"data/trec45_indri_kstem_top1000_bm25.out\", delimiter=\" \",names=['query','Q0','DOCNO','rank','bm25metric','bm25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = bm25[bm25[\"rank\"]<100]\n",
    "bm25 = bm25.drop(columns=['Q0','bm25metric','bm25'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25['present']=np.where((bm25['DOCNO'].isin(docs[\"DOCNO\"])), 1, 0)\n",
    "to_rank = bm25[bm25['present']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_queries = set(to_rank['query'])\n",
    "# print(len(unique_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rank_merged = to_rank.merge(docs, on=['DOCNO'], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rank_docs={}\n",
    "for query_id in unique_queries:\n",
    "    docs_=list(to_rank_merged[to_rank_merged[\"query\"]==query_id][\"TEXT\"])\n",
    "    to_rank_docs[query_id] = docs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text={}\n",
    "for query_id in unique_queries:\n",
    "    if query_id in queries.keys():\n",
    "        query_text[query_id] = queries[query_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### \n",
    "def make_bert_input(tokenizer,doctext):\n",
    "    bert_input = []\n",
    "    for text in doctext:\n",
    "        bert_input.append(tokenizer(text,return_tensors=\"pt\",max_length=500))\n",
    "    return bert_input\n",
    "    \n",
    "# print(len(bert_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_main_dict(query_text,model,bert_input,tokenizer,model_name):\n",
    "    main_dict=[]\n",
    "    if(model_name == \"IDCM\"):\n",
    "        query_input = tokenizer(query_text ,return_tensors=\"pt\",max_length=30,truncation=True)\n",
    "        for i, inp in enumerate(bert_input):\n",
    "            score  = model(query_input, inp).squeeze(0)\n",
    "            lst = (docnos[i],score.item())\n",
    "            main_dict.append(lst)\n",
    "\n",
    "    if(model_name==\"COLBERT\"):\n",
    "            \n",
    "        query_input = tokenizer(query_text)\n",
    "\n",
    "        query_input.input_ids += [103] * 8 # [MASK]\n",
    "        query_input.attention_mask += [1] * 8\n",
    "        query_input[\"input_ids\"] = torch.LongTensor(query_input.input_ids).unsqueeze(0)\n",
    "        query_input[\"attention_mask\"] = torch.LongTensor(query_input.attention_mask).unsqueeze(0)\n",
    "        for i, inp in enumerate(bert_input):\n",
    "            score  = model.forward(query_input, inp).squeeze(0)\n",
    "            lst = (docnos[i],score.item())\n",
    "            main_dict.append(lst)\n",
    "            \n",
    "    return main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "query_results_dict={}\n",
    "for query_id in query_text.keys():\n",
    "    query = query_text[query_id]\n",
    "    idcm_input=make_bert_input(idcm_tokenizer,to_rank_docs[query_id])\n",
    "    idcm_result_lst= make_main_dict(query,idcm_model,idcm_input,idcm_tokenizer,\"IDCM\")\n",
    "    query_results_dict[query_id] = idcm_result_lst\n",
    "\n",
    "idcm_ranked_results = unrolled_to_ranked_result(query_results_dict)\n",
    "idcm_metrics = calculate_metrics_plain(idcm_ranked_results,load_qrels(test_config[\"qrels\"]),test_config[\"binarization_point\"])\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idcm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "query_results_dict={}\n",
    "for query_id in query_text.keys():\n",
    "    query = query_text[query_id]\n",
    "    colbert_input=make_bert_input(colbert_tokenizer,to_rank_docs[query_id])\n",
    "    colbert_result_lst= make_main_dict(query,colbert_model,colbert_input,colbert_tokenizer,\"COLBERT\")\n",
    "    query_results_dict[query_id] = colbert_result_lst\n",
    "colbert_ranked_results = unrolled_to_ranked_result(query_results_dict)\n",
    "colbert_metrics = calculate_metrics_plain(colbert_ranked_results,load_qrels(test_config[\"qrels\"]),test_config[\"binarization_point\"])\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERTDOT and BERTCAT comparison \n",
    "\n",
    "# bertdot_input=make_bert_input(bertdot_tokenizer,texts)\n",
    "# bertdot_main_dict= make_main_dict(query_text,bertdot_model,bertdot_input)\n",
    "# bertdot_ranked_results = unrolled_to_ranked_result(bertdot_main_dict)\n",
    "# bertdot_metrics = calculate_metrics_plain(bertdot_ranked_results,load_qrels(test_config[\"qrels\"]),test_config[\"binarization_point\"])\n",
    "\n",
    "# colbert_input=make_bert_input(colbert_tokenizer,texts)\n",
    "# colbert_main_dict= make_main_dict(query_text,colbert_model,colbert_input)\n",
    "# colbert_ranked_results = unrolled_to_ranked_result(colbert_main_dict)\n",
    "# colbert_metrics = calculate_metrics_plain(colbert_ranked_results,load_qrels(test_config[\"qrels\"]),test_config[\"binarization_point\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
